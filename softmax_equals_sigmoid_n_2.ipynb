{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lecture notes claim that softmax is equivalent to sigmoid when n=2. He didn't prove or explain this further, so I thought I'd prove it here myself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So obviously they're not equal, what am I doing wrong? \n",
    "\n",
    "The first thing to emphasize is that when they say n=2, they don't mean n datapoints, they mean n parameters. I.e. they are talking about $\\theta$, not $X$.\n",
    "\n",
    "So here's what I'm going to do: \n",
    "\n",
    "1. Prove the result algebraically \n",
    "2. Prove the result numerically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[This answer on Quora seems to explain stuff pretty well!](https://www.quora.com/Why-is-it-better-to-use-Softmax-function-than-sigmoid-function)\n",
    "\n",
    "Here's the idea rewritten: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax definition: \n",
    "\n",
    "\\begin{align}\n",
    "P(y = k | X) = \\frac{\\exp({\\theta_j^T \\cdot X})}{\\sum_{i=0}^{n-1}{\\exp({\\theta_i^T \\cdot X})}} \n",
    "= \\frac{1}{\\sum_{i=0}^{n-1}{\\exp({(\\theta_i - \\theta_j)^T \\cdot X})}}\n",
    "= \\frac{1}{\\sum_{i=0}^{n-1}{\\exp(-{(\\theta_j - \\theta_i)^T \\cdot X})}}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "When n = 2,\n",
    "\\begin{align}\n",
    "P(y = 1 | X) = \\frac{1}{\\sum_{i=0}^{n-1}{\\exp(-{(\\theta_1 - \\theta_i)^T . X})}}\n",
    "= \\frac{1}{{\\exp(-{(\\theta_1 - \\theta_0)^T . X})}} + \\frac{1}{{\\exp(-{(\\theta_1 - \\theta_1)^T . X})}}\n",
    "= \\frac{1}{{\\exp(-{(\\theta_1 - \\theta_0)^T . X})} + 1}\n",
    "= \\sigma((\\theta_1 - \\theta_0)^T . X)\n",
    "\\end{align}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just a reminder: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "By linear regression, we say: \n",
    "\n",
    "\\begin{align}\n",
    "f(y | X_i) = \\sum_{j=0}^{n-1}{{\\theta_j^T \\cdot X_i}}\n",
    "\\end{align}\n",
    "\n",
    "For linear regression, we say: \n",
    "\n",
    "\\begin{align}\n",
    "logit(P(y=k | X_i)) = \\sum_{j=0}^{n-1}{{\\theta_j^T \\cdot X_i}}\n",
    "\\end{align}\n",
    "\n",
    "Where: \n",
    "\n",
    "\\begin{align}\n",
    "logit(x) = \\log\\frac{p}{1-p}\n",
    "\\end{align}\n",
    "\n",
    "So:\n",
    "\n",
    "\\begin{align}\n",
    "P(y=k | X_i) = \\frac{1}{1+\\exp(-y)}\n",
    "\\end{align}\n",
    "\n",
    "Where: \n",
    "\n",
    "\\begin{align}\n",
    "y = \\exp{\\sum_{j=0}^{n-1}{{\\theta_j^T \\cdot X_i}}}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now to prove numerically "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data_points, num_features, num_classes = 1000, 10, 2\n",
    "\n",
    "data_matrix = np.random.normal(size=(num_data_points, num_features))\n",
    "weights_matrix = np.random.random(size=(num_features, num_classes - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.25896617],\n",
       "       [-4.20362076],\n",
       "       [ 2.34005395],\n",
       "       [-1.51754527],\n",
       "       [ 1.04138182],\n",
       "       [ 1.89313601],\n",
       "       [-0.26774478],\n",
       "       [ 0.91482927],\n",
       "       [ 0.18728667],\n",
       "       [ 0.20086445]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.matmul(data_matrix, weights_matrix)\n",
    "z[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using sigmoid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43561786],\n",
       "       [0.01472142],\n",
       "       [0.91214041],\n",
       "       [0.17982328],\n",
       "       [0.73911654],\n",
       "       [0.86911268],\n",
       "       [0.43346083],\n",
       "       [0.71398736],\n",
       "       [0.54668528],\n",
       "       [0.55004795]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = np.exp(z) / (1 + np.exp(z))\n",
    "p[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_predictions = (p > 0.5).astype(int).reshape(1, -1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matrix_for_softmax = np.hstack([1 - p, p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_predictions = matrix_for_softmax.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 1, 1, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 1, 1, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid_predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ True]), array([1000]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(softmax_predictions == sigmoid_predictions, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
